{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78b6962",
   "metadata": {},
   "source": [
    "4. Finally, combine the time series information from the two datasets together to see how well correlated heavy job-completion load is with the unresponsiveness, and to see if there is some threshold of job completions per hour that generally results in unresponsiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d60185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, select, Table, MetaData\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mpl_dates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SQL engine for pulling down data and reflect tables.\n",
    "engine = create_engine('sqlite:///data/jobs.db')\n",
    "connection = engine.connect()\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "jobs = Table('jobs', metadata, autoload = True, autoload_with = engine)\n",
    "slurm = Table('slurm', metadata, autoload = True, autoload_with = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867acf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull initial columns from database for overlay on top of each other. i.e., jobs vs. unresponsiveness by time.\n",
    "end_times = pd.read_sql(\"SELECT STRFTIME('%Y-%m-%dT%H:%M:%S', END) AS END FROM jobs\",\n",
    "                        con = connection,\n",
    "                        parse_dates = ['END'])\n",
    "\n",
    "unresponsive = pd.read_sql(\"SELECT UNRESPONSIVE, STRFTIME('%Y-%m-%dT%H:%M:%S', DATETIME) AS DATETIME FROM slurm\", \n",
    "                           con = connection, \n",
    "                           parse_dates = ['DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71964f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each set of columns by week and merge on datetime.\n",
    "# Make pd.Grouper consistent on origin.\n",
    "end_times['HOURCOUNT'] = ''\n",
    "end_times = (\n",
    "    end_times.set_index('END').\\\n",
    "    resample('1H').\\\n",
    "    count().\\\n",
    "    reset_index().\\\n",
    "    rename(columns = {'END':'Hour',\n",
    "                     'HOURCOUNT': 'Completions Per Hour'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ffed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unresponsive = (\n",
    "    unresponsive.set_index('DATETIME').\\\n",
    "    groupby(pd.Grouper(freq = '1H'))['UNRESPONSIVE'].\\\n",
    "    agg(['sum']).\\\n",
    "    reset_index().\\\n",
    "    rename(columns = {'sum':'Unresponsive Counts',\n",
    "                     'DATETIME':'Hour'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unending = end_times.merge(unresponsive, on = 'Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd34f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hexbin chart showing negative correlation between Completions Per Hour and Unresponsive Counts.\n",
    "def thousands_commas_please(ax, *args):\n",
    "    if 'x' in args:\n",
    "        ax.get_xaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "    if 'y' in args:\n",
    "        ax.get_yaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "x = unending['Completions Per Hour']\n",
    "y = unending['Unresponsive Counts']\n",
    "\n",
    "def make_a_hexbin(x, y):\n",
    "    fig, ax = plt.subplots(figsize = (15, 10), dpi = 300, facecolor = 'k')\n",
    "    plt.hexbin(x = x, \n",
    "               y = y, \n",
    "               C = [i.isocalendar()[1] for i in unending['Hour']], \n",
    "               cmap = 'gray')\n",
    "    ax.set_facecolor('k')\n",
    "    ax.set_xlabel('')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    thousands_commas_please(ax, 'x')\n",
    "    ax.tick_params(colors ='white', which = 'both') \n",
    "\n",
    "make_a_hexbin(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32797863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroup completions per hour and unresponsive counts by week.\n",
    "column_list = ['Completions Per Hour','Unresponsive Counts']\n",
    "\n",
    "unending_by_week = (\n",
    "    unending.set_index('Hour').\\\n",
    "    groupby(pd.Grouper(freq = 'W'))[column_list].\\\n",
    "    agg(['sum']).\\\n",
    "    reset_index().\\\n",
    "    rename(columns = {'Hour':'Week',\n",
    "                     'Completions Per Hour': 'Completions Per Week'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minmax columns for fill between to show difference.\n",
    "unending_by_week['Completions Per Week MinMax'] = (\n",
    "    unending_by_week['Completions Per Week']['sum'].\\\n",
    "    apply(lambda x: (x - unending_by_week['Completions Per Week'].min())/\\\n",
    "         (unending_by_week['Completions Per Week'].max() - \\\n",
    "          unending_by_week['Completions Per Week'].min()))\n",
    ")\n",
    "\n",
    "unending_by_week['Unresponsive Counts MinMax'] = (\n",
    "    unending_by_week['Unresponsive Counts']['sum'].\\\n",
    "    apply(lambda x: (x - unending_by_week['Unresponsive Counts'].min())/\\\n",
    "         (unending_by_week['Unresponsive Counts'].max() - \\\n",
    "          unending_by_week['Unresponsive Counts'].min()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fill between plot showing relative difference between completions per week and unresponsive counts.\n",
    "facecolor = 'white'\n",
    "fig, ax = plt.subplots(figsize=(14,8), \n",
    "                       facecolor = facecolor,\n",
    "                      dpi = 200)\n",
    "\n",
    "y1 = unending_by_week['Completions Per Week MinMax']\n",
    "y0 = unending_by_week['Unresponsive Counts MinMax']\n",
    "x = unending_by_week['Week']\n",
    "\n",
    "ax.fill_between(x, y0, y1, facecolor = 'blue', alpha = 0.5)\n",
    "\n",
    "plt.xticks(x, rotation = 75, fontsize = 10)\n",
    "plt.xlabel('Week Ending')\n",
    "plt.ylabel('Difference Between Completions and Unresponsive Counts Per Week')\n",
    "plt.title('Difference Between Completions and Unresponsive Counts Per Week Over Time')\n",
    "ax.xaxis.set_major_formatter(mpl_dates.DateFormatter('%D'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider importing from the Question 1 Jupyter Notebook.\n",
    "# Pull down SQL columns.\n",
    "vague_memory = pd.read_sql(\"SELECT USEDMEMPERCORE, STRFTIME('%Y-%m-%dT%H:%M:%S', END) AS END FROM jobs\",\n",
    "                           con = connection,\n",
    "                           parse_dates = ['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group total and standard deviation of used memory per core per hour and save as a dataframe.\n",
    "vague_memory_by_hour = (\n",
    "    vague_memory.set_index('END').\\\n",
    "    groupby(pd.Grouper(freq = '1H'))['USEDMEMPERCORE'].\\\n",
    "    agg(['sum','std']).\\\n",
    "    reset_index().\\\n",
    "    rename(columns = {'END':'Hour',\n",
    "                     'sum':'Total Memory Per Core',\n",
    "                     'std':'Std. Dev. of Total Memory Per Core'}).\\\n",
    "    round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e51253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hexbin plotting memory variability per hour vs. unresponsive counts.\n",
    "memory_doesnt_respond = vague_memory_by_hour.merge(unending, on = 'Hour')\n",
    "x = memory_doesnt_respond['Std. Dev. of Total Memory Per Core']\n",
    "y = memory_doesnt_respond['Unresponsive Counts']\n",
    "make_a_hexbin(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caced946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the absolute value of the zscores of used memory per core.\n",
    "vague_memory['ABSUMPCZS'] = abs(zscore(vague_memory['USEDMEMPERCORE'], ddof = 1))\n",
    "\n",
    "# Group absolute value of zscores by hour then sum, mean for that timeframe.\n",
    "zscores_by_hour = (\n",
    "    vague_memory.set_index('END').\\\n",
    "    groupby(pd.Grouper(freq = 'H'))['ABSUMPCZS'].\\\n",
    "    agg(['sum','mean']).\\\n",
    "    reset_index().\\\n",
    "    rename(columns = {'END':'Hour',\n",
    "                      'sum':'Total Z-Score for Jobs That Hour',\n",
    "                      'mean':'Avg Z-Score for Jobs That Hour'}))\n",
    "\n",
    "# Create a total z-score for jobs in a hour vs. unresponsive counts.\n",
    "zscore_doesnt_respond = zscores_by_hour.merge(unending, on = 'Hour')\n",
    "zscore_series = zscore_doesnt_respond['Avg Z-Score for Jobs That Hour']\n",
    "uc_series = zscore_doesnt_respond['Unresponsive Counts']\n",
    "\n",
    "x_coords = zscore_series.apply(lambda x: (x - zscore_series.min()/(zscore_series.max() - zscore_series.min())))\n",
    "y_coords = uc_series.apply(lambda x: (x - uc_series.min()/(uc_series.max() - uc_series.min())))\n",
    "make_a_hexbin(x_coords, y_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull all variables that can be made into numeric variables for correlation heatmap purposes.\n",
    "pearson_pull = (\n",
    "    pd.read_sql(\"SELECT USEDMEMPERCORE,\\\n",
    "    REQMEMPERCORE,\\\n",
    "    REQTIME,\\\n",
    "    USEDTIME,\\\n",
    "    STRFTIME('%Y-%m-%dT%H:%M:%S', END) AS END FROM jobs\",\n",
    "                con = connection,\n",
    "                parse_dates = ['END'])\n",
    ")\n",
    "\n",
    "# Create dataframe of endtimes to merge in as well.\n",
    "endtimes = pearson_pull[['END']]\n",
    "endtimes['ENDCOUNTS'] = ''\n",
    "endtimes = endtimes.set_index('END').resample('1H').count().reset_index()\n",
    "\n",
    "pearson_pull_slurm = (\n",
    "    pd.read_sql(\"SELECT UNRESPONSIVE,\\\n",
    "    TESTING,\\\n",
    "    USERCOUNT,\\\n",
    "    STRFTIME('%Y-%m-%dT%H:%M:%S', DATETIME) AS DATETIME FROM slurm\",\n",
    "                con = connection,\n",
    "                parse_dates = ['DATETIME'])\n",
    ")\n",
    "\n",
    "# Sort the pull columns by hour.\n",
    "pearson_pull_by_hour = (\n",
    "    pearson_pull.set_index('END').\\\n",
    "    groupby(pd.Grouper(freq = 'H'))[[i for i in pearson_pull.columns if i != 'END']].\\\n",
    "    agg(['sum']).\\\n",
    "    reset_index())\n",
    "\n",
    "pearson_pull_slurm_by_hour = (\n",
    "    pearson_pull_slurm.set_index('DATETIME').\\\n",
    "    groupby(pd.Grouper(freq = 'H'))[[i for i in pearson_pull_slurm.columns if i != 'DATETIME']].\\\n",
    "    agg(['sum']).\\\n",
    "    reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General clean-up to get rid of multi-indexes.\n",
    "pearson_pull_slurm_by_hour.columns = pearson_pull_slurm_by_hour.columns.map('_'.join)\n",
    "\n",
    "pearson_pull_slurm_by_hour = (\n",
    "    pearson_pull_slurm_by_hour[['DATETIME_',\n",
    "                                'UNRESPONSIVE_sum',\n",
    "                                'TESTING_sum',\n",
    "                                'USERCOUNT_sum']].\\\n",
    "    rename(columns = {'DATETIME_':'DATETIME',\n",
    "                      'UNRESPONSIVE_sum':'UNRESPONSIVE',\n",
    "                      'TESTING_sum':'TESTING',\n",
    "                      'USERCOUNT_sum':'USERCOUNT'}))\n",
    "\n",
    "pearson_pull_by_hour.columns = pearson_pull_by_hour.columns.map('_'.join)\n",
    "\n",
    "pearson_pull_by_hour = (\n",
    "    pearson_pull_by_hour[['END_',\n",
    "                          'USEDMEMPERCORE_sum',\n",
    "                          'REQMEMPERCORE_sum',\n",
    "                          'REQTIME_sum',\n",
    "                          'USEDTIME_sum']].\\\n",
    "    rename(columns = {'END_':'END',\n",
    "                      'USEDMEMPERCORE_sum':'USEDMEMPERCORE',\n",
    "                      'REQMEMPERCORE_sum':'REQMEMPERCORE',\n",
    "                      'REQTIME_sum':'REQTIME',\n",
    "                      'USEDTIME_sum':'USEDTIME'}))\n",
    "\n",
    "# Merge each hour sorted data frame together and pull out time.\n",
    "ppmergeptone = pearson_pull_slurm_by_hour.merge(endtimes, left_on = 'DATETIME', right_on = 'END')\n",
    "ppmergeptone = ppmergeptone[[i for i in ppmergeptone.columns if i != 'DATETIME']]\n",
    "ppmerge = pearson_pull_by_hour.merge(ppmergeptone, on = 'END')\n",
    "ppmergewotime = ppmerge[['USEDMEMPERCORE',\n",
    "                         'REQMEMPERCORE',\n",
    "                         'REQTIME',\n",
    "                         'USEDTIME',\n",
    "                         'UNRESPONSIVE',\n",
    "                         'TESTING',\n",
    "                         'USERCOUNT',\n",
    "                         'ENDCOUNTS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later import due to effects on matplotlib.\n",
    "from yellowbrick.features import Rank2D\n",
    "\n",
    "# Plot Pearson correlation heatmap.\n",
    "plt.figure(dpi = 300)\n",
    "visualizer = Rank2D(algorithm = \"pearson\")\n",
    "visualizer.fit_transform(ppmergewotime)\n",
    "visualizer.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
